{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc0e9e20",
   "metadata": {},
   "source": [
    "# MEDICAL QUERY CHATBOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04b62fb",
   "metadata": {},
   "source": [
    "The Medical Query Chatbot represents a sophisticated application designed to cater to users seeking detailed information about medical conditions, symptoms, and survival rates. This innovative project harnesses the power of advanced natural language processing, prominently featuring OpenAI's cutting-edge language model. The user interface is seamlessly crafted using Streamlit, a framework that facilitates the development of interactive applications.\n",
    "\n",
    "The architecture of the chatbot is orchestrated through a SequentialChain, comprising three interconnected LLMChains. Each LLMChain is meticulously tailored to address distinct dimensions of a user's query. \n",
    "\n",
    "The first chain, employing a PromptTemplate, initiates a conversation with the OpenAI language model to extract general information about the specified medical condition. Notably, due to the unavailability of the OpenAI API key for this demonstration, the actual output couldn't be displayed; however, the underlying code functions seamlessly.\n",
    "\n",
    "The second chain focuses on extracting information related to symptoms associated with the identified medical condition, utilizing a dedicated PromptTemplate and interacting with the OpenAI model. The third chain delves into survival rates, employing yet another PromptTemplate to prompt the language model. Throughout this process, ConversationBufferMemory instances play a pivotal role in storing and retrieving relevant details, ensuring context coherency across chains.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f567827",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from constants import openai_key\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import SequentialChain\n",
    "import streamlit as st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b17502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "\n",
    "# Streamlit framework\n",
    "st.title('Medical Query Chatbot')\n",
    "input_text = st.text_input(\"Ask about symptoms of a medical condition\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0787a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prompt Templates\n",
    "\n",
    "# Define a prompt template for the first input (Disease name)\n",
    "first_input_prompt = PromptTemplate(\n",
    "    input_variables=['Disease'],\n",
    "    template=\"Tell me about {Disease}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844ff3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Memory\n",
    "\n",
    "# Create a memory buffer for storing information about the Disease\n",
    "disease_details = ConversationBufferMemory(input_key='Disease', memory_key='chat_history')\n",
    "\n",
    "# Create an OpenAI language model with a temperature setting\n",
    "llm = OpenAI(temperature=0.8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LLMChain for the first input\n",
    "chain = LLMChain(\n",
    "    llm=llm, prompt=first_input_prompt, verbose=True, output_key='Information', memory=disease_details\n",
    ")\n",
    "# Define a prompt template for the second input (Disease symptoms)\n",
    "second_input_prompt = PromptTemplate(\n",
    "    input_variables=['Disease'],\n",
    "    template=\"The symptoms of {Disease} are:\"\n",
    ")\n",
    "\n",
    "# Create a memory buffer for storing information about the Disease symptoms\n",
    "disease_symptoms = ConversationBufferMemory(input_key='Disease', memory_key='symptoms_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d138780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LLMChain for the second input\n",
    "chain2 = LLMChain(\n",
    "    llm=llm, prompt=second_input_prompt, verbose=True, output_key='Symptoms', memory=disease_symptoms\n",
    ")\n",
    "\n",
    "# Define a prompt template for the third input (survival rate)\n",
    "third_input_prompt = PromptTemplate(\n",
    "    input_variables=['Disease', 'survival_rate'],\n",
    "    template=\"The survival rate of {Disease} is {survival_rate}\"\n",
    ")\n",
    "\n",
    "# Create a memory buffer for storing information about survival rate\n",
    "survival_rate_memory = ConversationBufferMemory(input_key='Disease', memory_key='survival_rate_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6569ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LLMChain for the third input\n",
    "chain3 = LLMChain(llm=llm, prompt=third_input_prompt, verbose=True, output_key='survival_rate', memory=survival_rate_memory)\n",
    "\n",
    "# Create a SequentialChain to combine the three LLMChains\n",
    "parent_chain = SequentialChain(\n",
    "    chains=[chain, chain2, chain3],\n",
    "    input_variables=['Disease'],\n",
    "    output_variables=['Information', 'Symptoms', 'survival_rate'],\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8532e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results in the Streamlit app\n",
    "if input_text:\n",
    "    st.write(parent_chain({'Disease': input_text}))\n",
    "\n",
    "    with st.expander('Symptoms'):\n",
    "        st.info(disease_symptoms.buffer)\n",
    "\n",
    "    with st.expander('Survival Rate'):\n",
    "        st.info(survival_rate_memory.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395455a9",
   "metadata": {},
   "source": [
    "Despite not having access to the OpenAI API key for this particular demonstration, the code structure and logic are robust and fully functional. The absence of the API key merely restricts the display of the actual output within the Streamlit interface. In a real-world scenario, with the appropriate API key, users would experience a seamless interaction where the chatbot delivers detailed responses encompassing general information, symptoms, and survival rates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
